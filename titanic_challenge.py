# -*- coding: utf-8 -*-
"""Titanic_challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Y6v8WJKQ6oNhr_RyGbojTW5LNGFYk7R

# import dataset and libraries
"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score

dataset=pd.read_csv("https://raw.githubusercontent.com/ojal21/ML_Dataset/main/titanic_train.csv")

dataset.head()

dataset.isna().sum()

"""# EDA
Relationship bw features and survived
"""

survived=dataset[dataset['Survived']==1]
print(len(survived))
not_survived=dataset[dataset['Survived']==0]
print(len(not_survived))

#Pclass vs survived
pclass_survived = dataset.groupby('Pclass').Survived.value_counts()
pclass_survived

#sex vs survived
sex_survived = dataset.groupby('Sex').Survived.value_counts()
sex_survived

#embarked vs survived
embarked_survived=dataset.groupby('Embarked').Survived.value_counts()
embarked_survived

#parch vs survived
parch_survied=dataset.groupby('Parch').Survived.value_counts()
parch_survied

#sibsp vs survived
sib_survived=dataset.groupby('SibSp').Survived.value_counts()
sib_survived

"""# Feature Extraction

1. Name feature 
extract title and then group the less common ones; 
encode later
"""

dataset_test=pd.read_csv("https://raw.githubusercontent.com/ojal21/ML_Dataset/main/titanic_test.csv")
train_test=[dataset,dataset_test]

for row in train_test:
    row['Title'] = row['Name'].str.extract(' ([A-Za-z]+)\.')

pd.crosstab(dataset['Title'], dataset['Sex'])

for row in train_test:
    row['Title'] = row['Title'].replace(['Lady', 'Countess','Capt', 'Col', \
 	'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')

    row['Title'] = row['Title'].replace('Mlle', 'Miss')
    row['Title'] = row['Title'].replace('Ms', 'Miss')
    row['Title'] = row['Title'].replace('Mme', 'Mrs')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Other": 5}
for row in train_test:
    row['Title'] = row['Title'].map(title_mapping)
    row['Title'] = row['Title'].fillna(0)

"""2.Sex feature; encode"""

for row in train_test:
    row['Sex'] = row['Sex'].map( {'female': 1, 'male': 0} ).astype(int)

"""3. Embarked ; fill na with 'S' as max passengers ; encode"""

for row in train_test:
    row['Embarked'] = row['Embarked'].fillna('S')

for row in train_test:
    #print(row.Embarked.unique())
    row['Embarked'] = row['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

"""4. Age; fill na; group ; encode"""

for row in train_test:
    age_avg = row['Age'].mean()
    row['Age'][np.isnan(row['Age'])] = age_avg

dataset['AgeBand'] = pd.cut(dataset['Age'], 5)
print(dataset['AgeBand'])

for row in train_test:
    row.loc[ row['Age'] <= 16, 'Age'] = 0
    row.loc[(row['Age'] > 16) & (row['Age'] <= 32), 'Age'] = 1
    row.loc[(row['Age'] > 32) & (row['Age'] <= 48), 'Age'] = 2
    row.loc[(row['Age'] > 48) & (row['Age'] <= 64), 'Age'] = 3
    row.loc[ row['Age'] > 64, 'Age'] = 4

"""  5. Fare; divide into bands; encode"""

for row in train_test:
    row['Fare'] = row['Fare'].fillna(dataset['Fare'].median())

dataset['FareBand'] = pd.qcut(dataset['Fare'], 4)

for row in train_test:
    row.loc[ row['Fare'] <= 7.91, 'Fare'] = 0
    row.loc[(row['Fare'] > 7.91) & (row['Fare'] <= 14.454), 'Fare'] = 1
    row.loc[(row['Fare'] > 14.454) & (row['Fare'] <= 31), 'Fare']   = 2
    row.loc[ row['Fare'] > 31, 'Fare'] = 3
    row['Fare'] = row['Fare'].astype(int)

"""6. Sibparch and parch ; combine into family size """

for row in train_test:
    row['FamilySize'] = row['SibSp'] +  row['Parch'] + 1

"""7. creating extra feature ; is alone"""

for row in train_test:
    row['IsAlone'] = 0
    row.loc[row['FamilySize'] == 1, 'IsAlone'] = 1

"""# Feature selection"""

drop_feature=['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'FamilySize']
dataset=dataset.drop(drop_feature,axis=1)
dataset_test=dataset_test.drop(drop_feature,axis=1)
dataset=dataset.drop(['PassengerId', 'AgeBand', 'FareBand'], axis=1)

dataset.head()

dataset_test.head()

x_train=dataset.drop('Survived',axis=1)
y_train=dataset['Survived']
x_test=dataset_test.drop("PassengerId", axis=1).copy()

"""# Decision tree

"""

from sklearn.ensemble import RandomForestClassifier

gini=DecisionTreeClassifier( criterion='gini', max_depth=None)

#entropy=DecisionTreeClassifier(criterion="entropy",random_state=100,max_depth=3)

gini.fit(x_train,y_train)
entropy.fit(x_train,y_train)

y_pred_gini=gini.predict(x_test)

y_pred_entropy=entropy.predict(x_test)

acc_gini=accuracy_score(y_test,y_pred_gini)
acc_entropy=accuracy_score(y_test,y_pred_entropy)

acc_gini

acc_entropy

"""# Logistic regression

"""

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression(max_iter=300)

lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)

acc_lr=accuracy_score(y_test,y_pred_lr)
acc_lr

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=15)
knn.fit(x_train,y_train)
y_pred_knn=knn.predict(x_test)

acc_knn=accuracy_score(y_test,y_pred_lr)
acc_knn

"""# Naive bayes"""

from sklearn.naive_bayes import GaussianNB

nb=GaussianNB()
nb.fit(x_train,y_train)

y_pred_nb=nb.predict(x_test)

acc_nb=accuracy_score(y_test,y_pred_nb)
acc_nb

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf_gini=RandomForestClassifier(criterion='gini',n_estimators=200)
rf_gini.fit(x_train,y_train)
y_pred_rf_gini=rf_gini.predict(x_test)

y_pred_rf_gini

acc_rf_gini=accuracy_score(y_test,y_pred_rf_gini)
acc_rf_gini

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
gender_encoded = le.fit_transform(dataset_test['Sex'])
embarked_encoded=le.fit_transform(dataset_test['Embarked'])
dataset_test['encoded_gender'] = gender_encoded
dataset_test['encoded_embark']=embarked_encoded

dataset_test.head()

dataset_test.isna().sum()

dataset_test['Age']=dataset_test['Age'].replace(np.NaN,dataset_test['Age'].mean())
dataset_test['Fare']=dataset_test['Fare'].replace(np.NaN,dataset_test['Fare'].mean())

titanic_test=dataset_test.drop(columns=['Name','Ticket','Sex','Cabin','Embarked'])

y_titanic_pred=rf_gini.predict(x_test)

len(y_titanic_pred)

part1=pd.DataFrame(dataset_test['PassengerId'],columns=["PassengerId"])
part2=pd.DataFrame(y_pred_rf_gini,columns=["Survived"])

final=pd.concat([part1,part2],axis=1)
final.head()

final.to_csv(r'titanic_predicted.csv')

clf = RandomForestClassifier(n_estimators=5000,criterion='gini')
clf.fit(x_train, y_train)
y_pred_random_forest = clf.predict(x_test)
acc_random_forest = round(clf.score(x_train, y_train) * 100, 2)
print ("Train Accuracy: " + str(acc_random_forest) + '%')

"""# Gradient boosting"""

from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05,max_depth=None, random_state=0,min_samples_leaf=5).fit(x_train, y_train)

y_pred_gb = clf.predict(x_test)
acc_gb = round(clf.score(x_train, y_train) * 100, 2)
print ("Train Accuracy: " + str(acc_gb) + '%')

part1=pd.DataFrame(dataset_test['PassengerId'],columns=["PassengerId"])
part2=pd.DataFrame(y_pred_gb,columns=["Survived"])
final=pd.concat([part1,part2],axis=1)
final.head()
final.to_csv(r'titanic_predicted.csv')

"""# Ada Boost"""

from sklearn.ensemble import AdaBoostClassifier
clf_ada = AdaBoostClassifier(n_estimators=200, random_state=0,learning_rate=0.025)
clf_ada.fit(x_train,y_train)
y_pred_ada=clf.predict(x_test)
acc_ada = round(clf.score(x_train, y_train) * 100, 2)
print ("Train Accuracy: " + str(acc_ada) + '%')

part1=pd.DataFrame(dataset_test['PassengerId'],columns=["PassengerId"])
part2=pd.DataFrame(y_pred_ada,columns=["Survived"])
final=pd.concat([part1,part2],axis=1)
final.head()
final.to_csv(r'titanic_predicted.csv')